# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gyOh0SSB31b5o54SfL-5c30ENpzxFdDh
"""

# !pip install transformers==4.57.1

# !pip install tiktoken

# !pip install langchain-openai

from dotenv import load_dotenv
import os

load_dotenv() 

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
hf_key = os.getenv("HUGGINGFACEHUB_API_TOKEN")

assert hf_key is not None, "Missing HUGGINGFACEHUB_API_TOKEN"

from huggingface_hub import login
login(token=hf_key, add_to_git_credential=False)

import transformers
print(transformers.__version__)

import tiktoken
from datetime import datetime
from transformers import BartTokenizer, BartForConditionalGeneration, pipeline
import openai
import os
from langchain.chat_models import init_chat_model
import json
import re
from openai import OpenAI
from langchain_core.messages import HumanMessage
import spacy
nlp = spacy.load("en_core_web_sm")

import logging

logger = logging.getLogger("ambiguity_pipeline")
logger.setLevel(logging.INFO)

handler = logging.StreamHandler()
formatter = logging.Formatter(
    "[%(asctime)s] [%(levelname)s] %(message)s"
)
handler.setFormatter(formatter)

if not logger.handlers:
    logger.addHandler(handler)


def call_llm(model, prompt):
    response = model.invoke(prompt)
    print(response.content)
    return json.loads(response.content)

AMBIGUITY_KEYWORDS = {
    "pronouns": [
        "it", "this", "that", "these", "those", "they", "them"
    ],
    "indefinite": [
        "something", "someone", "somewhere", "some", "any",
        "others", "another", "other"
    ],
    "contextual": [
        "above", "below", "previous", "earlier", "later",
        "former", "latter"
    ],
    "vague_phrases": [
        "more information", "more details", "anything else",
        "something else", "in general", "in this context"
    ],
    "open_questions": [
        "what about", "what happened", "what does this mean"
    ],
    "comparison": [
        "better", "worse", "different", "similar", "more", "less"
    ],
    "abstract_nouns": [
        "thing", "stuff", "issue", "matter", "aspect",
        "situation", "case", "topic", "subject"
    ],
    "discourse": [
        "again", "also", "as mentioned", "as discussed",
        "like before", "as above"
    ]
}

"""# load config"""

def load_config(path="config.json"):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

"""# init memory"""

def init_session_memory(config):
    session = {
        "metadata": {
            "session_id": "1",
            "created_at": str(datetime.now()),
            "last_activity_at": str(datetime.now()),
            "ttl_seconds": 36000,
            "status": "active",
            "token_budget": config.get("token_budget"),
            "char_budget": config.get("char_budget")
        },
        "session_summary": {
            "user_profile": {"prefs": [], "constraints": []},
            "key_facts": [],
            "decisions": [],
            "open_questions": [],
            "todos": []
        },
        "message_range_summarized": {"from": None, "to": None},
        "working_messages": []
    }
    return session

def init_query_state(session_id, message_id, user_query):
    session = {
        "session_id": session_id,
        "message_id": message_id,
        "timestamp": str(datetime.now()),
        "original_query": user_query,
        "analysis": {
            "is_ambiguous": None,
            "ambiguity_types": [],
            "source": "",
            "bert_score": None,
            "rule_score": None
        },
        "rewriting": {
            "rewritten_query": "",
            "rewrite_strategy": []
        },
        "memory_usage": {
            "needed_context_keys": [],
            "memory_rationale": ""
        },
        "clarification": {
            "clarifying_questions": [],
            "best_question": "",
            "question_generation_policy": "",
            "max_clarification_turns": 0
        },
        "final_augmented_context": None
        }
    return session

import json
from pathlib import Path

def save_query_state(query_state, filepath="query_states.json"):
    # Nếu file chưa tồn tại thì tạo mới
    if not Path(filepath).exists():
        with open(filepath, "w") as f:
            json.dump([], f)

    # Đọc file hiện tại
    with open(filepath, "r") as f:
        data = json.load(f)

    # Append query_state mới
    data.append(query_state)

    # Ghi lại
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2)

def add_message(session, role, message, system_prompt_summary, model_summary, model_schema):
    new_id = len(session["working_messages"]) + 1

    clarification_msg = {
        "id": new_id,
        "role": role,
        "content": message
    }

    session["working_messages"].append(clarification_msg)

    conversation_text = convert_log_to_text(session["working_messages"])

    if should_trigger_summary(session, conversation_text):
        summary_text = bart_summarize(model_summary, conversation_text)

        print(f"Generated summary: {summary_text}")

        prompt = build_prompt_extract_schema(summary_text, system_prompt_summary)
        session_summary_temp = call_llm(model_schema, prompt)

        session_summary_mormalize = normalize_session_summary(session_summary_temp)
        session["session_summary"] = merge_summary(session["session_summary"], session_summary_mormalize)
        session["message_range_summarized"] = {"from": session["working_messages"][0]["id"], "to": session["working_messages"][-1]["id"]}
        session["working_messages"] = []

    save_session(session)

    return session

def pipeline_ambiguity(session, user_message, model_classifier, model_rewrite, model_clarification):
    session_id = session["metadata"]["session_id"]
    # msg_id = user_message["id"]
    if len(session["working_messages"]) == 0:
        msg_id = 0
    else:
        msg_id = session["working_messages"][-1]["id"]
    status = None

    logger.info(f"Start pipeline | session_id={session_id} | msg_id={msg_id}")

    query_state = init_query_state(session_id, msg_id, user_message["content"])
    logger.debug("Initialized query_state")

    query_state_checked = process_check_query(query_state, model_classifier)
    is_ambiguous = query_state_checked["analysis"]["is_ambiguous"]

    logger.info(f"Ambiguity check → {is_ambiguous}")
    logger.info(
        f"Source → {query_state_checked['analysis']['source']}"
    )

    if is_ambiguous:
        logger.info("Ambiguous query detected → rewrite flow")

        query_state["memory_usage"]["needed_context_keys"] = [
            "session_summary.key_facts",
            "session_summary.open_questions"
        ]
        logger.debug(f"Memory keys needed: {query_state['memory_usage']['needed_context_keys']}")

        query_state = build_final_augmented_context(session, query_state, window_size=3)
        logger.debug("Augmented context built")

        prompt, strategy = build_rewrite_prompt(query_state)
        logger.info(f"Rewrite strategy={strategy}")
        logger.debug(f"Rewrite prompt preview: {prompt[:300]}")

        res_rewritten = call_llm(model_rewrite, prompt)
        logger.debug(f"Rewrite raw output: {res_rewritten}")

        query_state = update_query_state_with_rewrite(query_state, res_rewritten, strategy)
        rewritten_query = query_state["rewriting"]["rewritten_query"]
        logger.info(f"Rewritten query: {rewritten_query}")

        rewrite_check = detect_ambiguity(rewritten_query, model_classifier=model_classifier)
        logger.info(f"Ambiguity after rewrite → {rewrite_check['is_ambiguous']}")

        if rewrite_check["is_ambiguous"]:
            logger.warning("Rewrite still ambiguous → clarification step")

            prompt_clarification = build_clarification_prompt(query_state)
            logger.debug(f"Clarification prompt preview: {prompt_clarification[:300]}")

            res_clarification = call_llm(model_clarification, prompt_clarification)
            logger.debug(f"Clarification raw output: {res_clarification}")

            query_state = update_clarification_question(query_state, res_clarification)
            logger.info("Clarification question generated")

            role = "assistant"
            status = "clarification"
            best_question = query_state["clarification"]["best_question"]

            print("Clarification question to user:", best_question)

        else:
            logger.info("Rewrite no longer ambiguous → no need for clarification")
            status = "rewrite_clear"
            role = "user"

    else:
        status = "clear"
        role = "user"

    logger.debug("Message added to session")

    save_query_state(query_state)
    logger.debug("Query state saved")

    logger.info("End pipeline")
    return query_state, status

"""# ambiguous"""

def rule_based_check(query: str, session_context=None):
    issues = []
    score = 0
    q_lower = query.lower()

    # --- Rule 1: Quá ngắn ---
    if len(query.split()) < 3:
        issues.append("too_short")
        score += 2

    # --- Rule 2: Match với AMBIGUITY_KEYWORDS ---
    for category, keywords in AMBIGUITY_KEYWORDS.items():
        for kw in keywords:
            if kw in q_lower:
                issues.append(f"keyword_{category}")
                score += 2
                break  # tránh duplicate trong cùng category

    # --- Rule 3: Regex cho câu hỏi yes/no thiếu subject ---
    if re.match(r"^(is it good\??|ok\??)$", q_lower):
        issues.append("missing_subject")
        score += 2

    # --- Rule 4: Phân tích cú pháp với spaCy ---
    doc = nlp(query)
    has_subject = any(token.dep_ in ["nsubj", "nsubjpass", "attr", "ccomp"] for token in doc)
    has_object = any(token.dep_ in ["dobj", "pobj"] for token in doc)
    has_main_verb = any(token.pos_ == "VERB" and token.dep_ == "ROOT" for token in doc)

    if not has_subject:
        issues.append("missing_subject")
        score += 1
    if not has_object:
        issues.append("missing_object")
        score += 1
    if not has_main_verb:
        issues.append("missing_main_verb")
        score += 1

    # --- Rule 5: Coreference nhưng không có antecedent trong session ---
    if any(pronoun in q_lower for pronoun in AMBIGUITY_KEYWORDS["pronouns"]):
        if not session_context or "entities" not in session_context:
            issues.append("unresolved_coreference")
            score += 1

    # Chuẩn hóa về thang 0–1
    normalized_score = score / 10

    return {
        "is_ambiguous": score > 3,
        "ambiguity_types": issues,
        "source": "rule",
        "rule_score": normalized_score
    }

def bert_classifier(query, model_classifier):
    result = model_classifier(query)
    label = result[0]['label']
    score = result[0]['score']
    is_ambiguous = not (label == "clear")
    return {
        "is_ambiguous": is_ambiguous,
        "ambiguity_types": ["bert_detect"],
        "source": "bert",
        "bert_score": score
    }

def detect_ambiguity(query: str, session_context=None, model_classifier=None) -> dict:
    # Step 1: Rule-based
    rule_result = rule_based_check(query, session_context)
    if rule_result["is_ambiguous"]:
        return {
            "is_ambiguous": True,
            "ambiguity_types": rule_result["ambiguity_types"],
            "source": "rule",
            "rule_score": rule_result["rule_score"]
        }

    # Step 2: BERT
    bert_result = bert_classifier(query, model_classifier)
    return {
        "is_ambiguous": bert_result["is_ambiguous"],
        "ambiguity_types": "bert_detect",
        "source": "bert",
        "bert_score": bert_result["bert_score"]
    }

def process_check_query(query_state, model_classifier, session_context=None):
    result = detect_ambiguity(
        query_state["original_query"],
        session_context, model_classifier
    )

    query_state["analysis"].update(result)

    return query_state

def build_final_augmented_context(session, query_state, window_size):
    needed_keys = query_state["memory_usage"].get("needed_context_keys", [])
    session_summary = session.get("session_summary", {})
    working_messages = session.get("working_messages", [])

    session_memory = {}
    for key in needed_keys:
        parts = key.split(".")
        ref = session_summary
        for p in parts[1:]:
            if isinstance(ref, dict) and p in ref:
                ref = ref[p]
            else:
                ref = None
                break
        if ref:
            session_memory[p] = ref

    recent_messages = []
    if working_messages:
        sorted_msgs = sorted(working_messages, key=lambda m: int(m["id"]), reverse=True)
        recent_messages = sorted_msgs[:window_size]
        recent_messages = list(reversed(recent_messages))

    query_state["final_augmented_context"] = {
        "recent_messages": recent_messages,
        "session_memory": session_memory
    }

    return query_state

def build_rewrite_prompt(query_state):
    """
    Xây dựng prompt cho LLM để rewrite query dựa trên final_augmented_context và original_query.
    """

    original_query = query_state["original_query"]
    augmented_context = query_state.get("final_augmented_context", {})

    recent_messages = augmented_context.get("recent_messages", [])
    session_memory = augmented_context.get("session_memory", {})

    # Xác định chiến lược rewrite
    if recent_messages and session_memory:
        strategy = "combine_summary_and_working_messages"
    elif recent_messages:
        strategy = "use_working_messages"
    elif session_memory:
        strategy = "use_session_summary"
    else:
        strategy = "no_context"

    # Xây dựng prompt cho LLM
    prompt = f"""
You are tasked with rewriting an ambiguous user query into a clear, unambiguous form.

Original Query:
"{original_query}"

Augmented Context:
- Recent Messages: {recent_messages}
- Session Memory: {session_memory}

Rules:
- Use ONLY information explicitly present in the augmented context.
- Do NOT introduce new facts, assumptions, or details.

Task:
1. Rewrite the query using the augmented context so it becomes unambiguous.
2. Provide the rewritten query in JSON format:

{{
  "rewritten_query": "..."
}}
"""

    return prompt, strategy

def update_query_state_with_rewrite(query_state, llm_output, strategy):
    """
    Cập nhật query_state["rewriting"] từ output của LLM.
    llm_output giả định là dict với keys: rewritten_query, rewrite_strategy, explanation.
    """
    query_state["rewriting"]["rewritten_query"] = llm_output.get("rewritten_query", "")
    query_state["rewriting"]["rewrite_strategy"] = strategy
    return query_state

def build_clarification_prompt(query_state):
    prompt_clarification = f"""
    You are tasked with generating clarifying questions for an ambiguous query.

    Rewritten Query: "{query_state["rewriting"]["rewritten_query"]}"
    Ambiguity Types: {query_state["analysis"]["ambiguity_types"]}

    Augmented Context:
    - Recent Messages: {query_state["final_augmented_context"]["recent_messages"]}
    - Session Memory: {query_state["final_augmented_context"]["session_memory"]}

    Rules:
    - Use ONLY information explicitly present in the augmented context.
    - Do NOT introduce new facts, assumptions, or details.

    Task:
    1. Generate 1 to 3 clarifying questions that would help the user specify their meaning.
    2. Rank the questions by usefulness (1 = most useful).
    3. Select the single best question to ask the user.
    4. Return in JSON format:
    {{
      "clarifying_questions": [
        {{"question": "...", "rank": 1}},
        {{"question": "...", "rank": 2}}
      ],
      "best_question": "..."
    }}
    """
    return prompt_clarification

def update_clarification_question(query_state, clar_question):
    """
    Cập nhật query_state["rewriting"] từ output của LLM.
    llm_output giả định là dict với keys: rewritten_query, rewrite_strategy, explanation.
    """
    query_state["clarification"]["clarifying_questions"] = clar_question.get("clarifying_questions", "")
    query_state["clarification"]["best_question"] = clar_question.get("best_question", "")
    return query_state

"""# summary"""

encoding = tiktoken.encoding_for_model("gpt-4o-mini")

def convert_log_to_text(messages):
    return "\n".join([f"{m['role']}: {m['content']}" for m in messages])

# Đếm token
def count_tokens(text):
    return len(encoding.encode(text))

# Đếm ký tự
def count_chars(text):
    return len(text)

def should_trigger_summary(session, conversation_text):
    char_count = count_chars(conversation_text) # Calculate here
    token_count = count_tokens(conversation_text) # Calculate here
    print(f"Context size  → {char_count} chars, {token_count} tokens")

    if char_count > session['metadata']['char_budget']:
        add_decision(session, "heuristic-based counting")
        return True

    if token_count > session['metadata']['token_budget']:
        add_decision(session, "tokenizer-based counting")
        return True

    return False

def bart_summarize(model_summary, conversation_text):
    result = model_summary(conversation_text)
    summary_text = result[0]["summary_text"]
    return summary_text

def build_prompt_extract_schema(summary_text, system_prompt):
    prompt = f"""{system_prompt}
    Summary: {summary_text}.
    Schema: {{ "user_profile": {{ "prefs": [], "constraints": [] }}, "key_facts": [], "open_questions": [], "todos": [] }}
    Field definitions:
    - user_profile.prefs:
      Temporary user preferences expressed or implied during this session
      (e.g., preferred explanation style, technical depth, language).
    - user_profile.constraints:
      Explicit requirements or limitations stated by the user
      (e.g., "do not collect data to driver", "must be fully distributed").

    - key_facts:
      Established facts or knowledge that were explicitly stated or clearly agreed upon
      during the conversation.
      These are considered true within this session and should not require re-explanation.

    - open_questions:
      Questions, uncertainties, or issues that were raised in the conversation
      but have not yet been answered or resolved.

    - todos:
      Concrete next actions or tasks that should be performed after the current step
      (e.g., writing code, comparing approaches, preparing documentation).

    Rules:
    - Do NOT invent or infer information that is not present in the summary.
    - Do NOT move the same information into multiple fields.
    - If a field has no applicable items, return an empty list.
    - Output valid JSON only. No explanations or comments.

    Example:
    {{
      "session_summary": {{
        "summary_text": "The user is designing a short-term session memory schema for a conversational LLM system. The discussion focuses on compressing conversation context into structured components instead of free-form text, including user preferences, established facts, architectural decisions, unresolved questions, and planned next steps.",
        "user_profile": {{
          "prefs": [
            "Prefer concise explanations",
            "Use technical and precise language",
            "Focus on system architecture rather than low-level implementation"
          ],
          "constraints": [
            "Do not use for-loops in examples",
            "Avoid using collect() in Spark-based discussions",
            "Assume a limited LLM context window"
          ]
        }},
        "key_facts": [
          "Session memory is constrained by the LLM token window",
          "Context summarization is triggered when the token limit is exceeded",
          "The system follows a conversational LLM architecture"
        ],
        "open_questions": [
          "How should the session summary be updated incrementally after each conversation turn?",
          "What is the optimal balance between structured memory and raw conversation messages?"
        ],
        "todos": [
          "Write pseudo-code for the session memory update pipeline",
          "Compare session memory with Retrieval-Augmented Generation (RAG)",
          "Prepare presentation slides describing the memory architecture"
        ]
      }},
    }}
    """
    return prompt

def normalize_session_summary(llm_output: dict):
    """
    Chuẩn hóa output LLM theo schema session_summary
    """
    summary = llm_output.get("session_summary", llm_output)

    return {
        "user_profile": {
            "prefs": summary.get("user_profile", {}).get("prefs", []),
            "constraints": summary.get("user_profile", {}).get("constraints", [])
        },
        "key_facts": summary.get("key_facts", []),
        "decisions": summary.get("decisions", []),
        "open_questions": summary.get("open_questions", []),
        "todos": summary.get("todos", [])
    }

def merge_summary(old_summary, new_summary):
    # user_profile
    for field in ["prefs", "constraints"]:
        old_vals = set(old_summary["user_profile"].get(field, []))
        new_vals = set(new_summary.get("user_profile", {}).get(field, []))
        old_summary["user_profile"][field] = list(old_vals.union(new_vals))

    # key_facts
    old_facts = set(old_summary.get("key_facts", []))
    new_facts = set(new_summary.get("key_facts", []))
    old_summary["key_facts"] = list(old_facts.union(new_facts))

    # decisions
    old_decisions = old_summary.get("decisions", [])
    new_decisions = new_summary.get("decisions", [])
    for d in new_decisions:
        if d not in old_decisions:
            old_decisions.append(d)
    old_summary["decisions"] = old_decisions

    # open_questions
    old_questions = set(old_summary.get("open_questions", []))
    new_questions = set(new_summary.get("open_questions", []))
    # remove answered questions
    answered = new_summary.get("answered_questions", [])
    old_questions = old_questions.difference(answered)
    # add new ones
    old_summary["open_questions"] = list(old_questions.union(new_questions))

    # todos
    old_todos = set(old_summary.get("todos", []))
    new_todos = set(new_summary.get("todos", []))
    completed = new_summary.get("completed_todos", [])
    old_todos = old_todos.difference(completed)
    old_summary["todos"] = list(old_todos.union(new_todos))

    return old_summary

def add_decision(session, decision_text):
    if decision_text not in session["session_summary"]["decisions"]:
        session["session_summary"]["decisions"].append(decision_text)
    session["metadata"]["last_activity_at"] = str(datetime.now())

SESSION_DIR = "sessions"
os.makedirs(SESSION_DIR, exist_ok=True)

def save_session(session):
    session_id = session["metadata"]["session_id"]
    path = os.path.join(SESSION_DIR, f"session_{session_id}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(session, f, ensure_ascii=False, indent=2)
    print(f"Session {session_id} saved.")

def load_session(session):
    path = os.path.join(SESSION_DIR, f"session_{session}.json")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Session {session} not found")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def build_context(session, system_prompt, message):
    parts = []

    # System prompt
    if system_prompt:
        parts.append(f"[System Instruction]\n{system_prompt}")

    # User message hiện tại
    parts.append("[Current User Question]\n" + message)

    # User profile
    prefs = session["session_summary"]["user_profile"].get("prefs", [])
    constraints = session["session_summary"]["user_profile"].get("constraints", [])
    if prefs or constraints:
        profile_text = []
        if prefs:
            profile_text.append("Preferences: " + ", ".join(prefs))
        if constraints:
            profile_text.append("Constraints: " + ", ".join(constraints))
        parts.append("[User Information]\n" + "\n".join(profile_text))

    # Key facts
    key_facts = session["session_summary"].get("key_facts", [])
    if key_facts:
        parts.append("[Key Facts Known]\n" + "\n".join(key_facts))

    # Decisions
    decisions = session["session_summary"].get("decisions", [])
    if decisions:
        parts.append("[Decisions]\n" + "\n".join(decisions))

    # Open questions
    open_questions = session["session_summary"].get("open_questions", [])
    if open_questions:
        parts.append("[Open Questions]\n" + "\n".join(open_questions))

    # Todos
    todos = session["session_summary"].get("todos", [])
    if todos:
        parts.append("[Todos]\n" + "\n".join(todos))

    # Working messages
    log_text = convert_log_to_text(session["working_messages"])
    if log_text.strip():
        parts.append("[Recent conversations]\n" + log_text)

    parts.append("[End of Context]")
    # Ghép toàn bộ thành full_context
    full_context = "\n\n".join(parts)
    return full_context

"""# Final"""

def generate_answer(session, query_text, system_prompt, system_prompt_summary, model_summary, model_schema, model_answer):
    full_context = build_context(session, system_prompt, query_text)
    response = model_answer.invoke(full_context)
    res_answer = response.content
    session = add_message(session, "assistant", res_answer, system_prompt_summary, model_summary, model_schema)
    return res_answer, session

import logging

def run_chat():
    config = load_config()
    session = init_session_memory(config)

    model_classifier = pipeline("text-classification", model="hoaan/bert-ambiguous-query-detector")
    model_summary = pipeline("summarization", model="luisotorres/bart-finetuned-samsum")
    model_rewrite = init_chat_model(
        "gpt-5-nano",
        model_provider="openai",
        temperature=0
    )
    model_clarification = init_chat_model(
        "gpt-4o-mini",
        model_provider="openai",
        temperature=0
    )
    model_answer = model_clarification
    model_schema = model_rewrite

    system_prompt_summary = "You are a dialogue state extractor. Given the following conversation summary, extract the dialogue state into the predefined JSON schema."
    system_prompt = "You are a helpful AI assistant. Answer the user's question clearly and concisely"

    print("=== Demo Chat Assistant ===")
    print("Type 'Exit' to exit.\n")

    while True:
        user_input = input("User: ")
        if user_input.lower().strip() == "exit":
            print("End demo.")
            break

        # Tạo message object
        user_message = {"id": len(session["working_messages"]) + 1,
                        "role": "user",
                        "content": user_input}

        # Lưu message gốc vào session
        session = add_message(session, "user", user_message["content"],
                              system_prompt_summary, model_summary, model_schema)

        # Chạy pipeline ambiguity
        query_state, status = pipeline_ambiguity(session, user_message, model_classifier, model_rewrite, model_clarification)

        # Phân nhánh xử lý
        if status == "clear":
            answer, session = generate_answer(session, user_message["content"],
                                              system_prompt, system_prompt_summary,
                                              model_summary, model_schema, model_answer)
            print("Assistant:", answer)

        elif status == "rewrite_clear":
            rewritten_query = query_state["rewriting"]["rewritten_query"]
            session = add_message(session, "user", rewritten_query,
                                  system_prompt_summary, model_summary, model_schema)
            answer, session = generate_answer(session, rewritten_query,
                                              system_prompt, system_prompt_summary,
                                              model_summary, model_schema, model_answer)
            print("Assistant:", answer)

        elif status == "clarification":
            best_question = query_state["clarification"]["best_question"]
            session = add_message(session, "assistant", best_question,
                                  system_prompt_summary, model_summary, model_schema)
            print("Assistant (clarification):", best_question)

        save_query_state(query_state)

if __name__ == "__main__":
    run_chat()
# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_5V6c86Os0y2CAE8OT6pdOJPFngyLxT

# Requirement
"""

# !pip install langgraph langgraph-checkpoint-sqlite

# !pip install transformers==4.57.1

# !pip install tiktoken

# !pip install langchain-openai

# !pip install python-dotenv

"""# Library"""

import tiktoken
from dotenv import load_dotenv
import os
from huggingface_hub import login
import transformers
from datetime import datetime
from transformers import BartTokenizer, BartForConditionalGeneration, pipeline
import openai
from langchain.chat_models import init_chat_model
import json
import re
from openai import OpenAI
import spacy
nlp = spacy.load("en_core_web_sm")

import logging

from typing import Annotated, TypedDict, List, Optional, Dict, Literal

from langgraph.types import Command
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.sqlite import SqliteSaver
import sqlite3
from langgraph.graph.message import add_messages, RemoveMessage
from pathlib import Path


load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
hf_api_key = os.getenv("HUGGINGFACEHUB_API_TOKEN")

print("OpenAI Key:", openai_api_key[:5] + "...")
print("HF Key:", hf_api_key[:5] + "...")


from openai import OpenAI
client = OpenAI(api_key=openai_api_key)

from huggingface_hub import InferenceClient
client = InferenceClient(token=hf_api_key)

"""# Init"""

encoding = tiktoken.encoding_for_model("gpt-4o-mini")

model_classifier = pipeline("text-classification", model="hoaan/bert-ambiguous-query-detector")
model_summary = pipeline("summarization", model="luisotorres/bart-finetuned-samsum")
model_rewrite = init_chat_model(
    "gpt-5-nano",
    model_provider="openai",
    temperature=0
)
model_clarification = init_chat_model(
    "gpt-4o-mini",
    model_provider="openai",
    temperature=0
)
model_answer = model_clarification
model_schema = model_rewrite

"""# Function Support"""

system_prompt = "You are a helpful AI assistant. Answer the user's question clearly and concisely"

def load_config(path="config.json"):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def convert_log_to_text(messages):
    lines = []
    for m in messages:
        if isinstance(m, HumanMessage):
            role = "user"
        elif isinstance(m, AIMessage):
            role = "assistant"
        elif isinstance(m, SystemMessage):
            role = "system"
        else:
            role = getattr(m, "type", "unknown")
        lines.append(f"{role}: {m.content}")
    return "\n".join(lines)

def count_tokens(text):
    return len(encoding.encode(text))

def count_chars(text):
    return len(text)

def add_decision(session, decision_text):
    if decision_text not in session["session_summary"]["decisions"]:
        session["session_summary"]["decisions"].append(decision_text)
    session["metadata"]["last_activity_at"] = str(datetime.now())

def should_trigger_summary(session, conversation_text):
    char_count = count_chars(conversation_text) # Calculate here
    token_count = count_tokens(conversation_text) # Calculate here
    print(f"Context size  → {char_count} chars, {token_count} tokens")

    if char_count > session['metadata']['char_budget']:
        add_decision(session, "heuristic-based counting")
        return True

    if token_count > session['metadata']['token_budget']:
        add_decision(session, "tokenizer-based counting")
        return True

    return False

def bart_summarize(model_summary, conversation_text):
    result = model_summary(conversation_text)
    summary_text = result[0]["summary_text"]
    print("Generated Summary:", summary_text)
    return summary_text

def call_llm(model, prompt):
    response = model.invoke(prompt)
    print(response.content)
    return json.loads(response.content)

def build_prompt_extract_schema(summary_text):
    prompt = f"""You are a dialogue state extractor. Given the following conversation summary, extract the dialogue state into the predefined JSON schema.
    Summary: {summary_text}.
    Schema: {{ "user_profile": {{ "prefs": [], "constraints": [] }}, "key_facts": [], "open_questions": [], "todos": [] }}
    Field definitions:
    - user_profile.prefs:
      Temporary user preferences expressed or implied during this session
      (e.g., preferred explanation style, technical depth, language).
    - user_profile.constraints:
      Explicit requirements or limitations stated by the user
      (e.g., "do not collect data to driver", "must be fully distributed").

    - key_facts:
      Established facts or knowledge that were explicitly stated or clearly agreed upon
      during the conversation.
      These are considered true within this session and should not require re-explanation.

    - open_questions:
      Questions, uncertainties, or issues that were raised in the conversation
      but have not yet been answered or resolved.

    - todos:
      Concrete next actions or tasks that should be performed after the current step
      (e.g., writing code, comparing approaches, preparing documentation).

    Rules:
    - Do NOT invent or infer information that is not present in the summary.
    - Do NOT move the same information into multiple fields.
    - If a field has no applicable items, return an empty list.
    - Output valid JSON only. No explanations or comments.

    Example:
    {{
      "session_summary": {{
        "summary_text": "The user is designing a short-term session memory schema for a conversational LLM system. The discussion focuses on compressing conversation context into structured components instead of free-form text, including user preferences, established facts, architectural decisions, unresolved questions, and planned next steps.",
        "user_profile": {{
          "prefs": [
            "Prefer concise explanations",
            "Use technical and precise language",
            "Focus on system architecture rather than low-level implementation"
          ],
          "constraints": [
            "Do not use for-loops in examples",
            "Avoid using collect() in Spark-based discussions",
            "Assume a limited LLM context window"
          ]
        }},
        "key_facts": [
          "Session memory is constrained by the LLM token window",
          "Context summarization is triggered when the token limit is exceeded",
          "The system follows a conversational LLM architecture"
        ],
        "open_questions": [
          "How should the session summary be updated incrementally after each conversation turn?",
          "What is the optimal balance between structured memory and raw conversation messages?"
        ],
        "todos": [
          "Write pseudo-code for the session memory update pipeline",
          "Compare session memory with Retrieval-Augmented Generation (RAG)",
          "Prepare presentation slides describing the memory architecture"
        ]
      }},
    }}
    """
    return prompt

def merge_summary(old_summary, new_summary):
    # user_profile
    for field in ["prefs", "constraints"]:
        old_vals = set(old_summary["user_profile"].get(field, []))
        new_vals = set(new_summary.get("user_profile", {}).get(field, []))
        old_summary["user_profile"][field] = list(old_vals.union(new_vals))

    # key_facts
    old_facts = set(old_summary.get("key_facts", []))
    new_facts = set(new_summary.get("key_facts", []))
    old_summary["key_facts"] = list(old_facts.union(new_facts))

    # decisions
    old_decisions = old_summary.get("decisions", [])
    new_decisions = new_summary.get("decisions", [])
    for d in new_decisions:
        if d not in old_decisions:
            old_decisions.append(d)
    old_summary["decisions"] = old_decisions

    # open_questions
    old_questions = set(old_summary.get("open_questions", []))
    new_questions = set(new_summary.get("open_questions", []))
    # remove answered questions
    answered = new_summary.get("answered_questions", [])
    old_questions = old_questions.difference(answered)
    # add new ones
    old_summary["open_questions"] = list(old_questions.union(new_questions))

    # todos
    old_todos = set(old_summary.get("todos", []))
    new_todos = set(new_summary.get("todos", []))
    completed = new_summary.get("completed_todos", [])
    old_todos = old_todos.difference(completed)
    old_summary["todos"] = list(old_todos.union(new_todos))

    return old_summary

AMBIGUITY_KEYWORDS = {
    "pronouns": [
        "it", "this", "that", "these", "those", "they", "them"
    ],
    "indefinite": [
        "something", "someone", "somewhere", "some", "any",
        "others", "another", "other"
    ],
    "contextual": [
        "above", "below", "previous", "earlier", "later",
        "former", "latter"
    ],
    "vague_phrases": [
        "more information", "more details", "anything else",
        "something else", "in general", "in this context"
    ],
    "open_questions": [
        "what about", "what happened", "what does this mean"
    ],
    "comparison": [
        "better", "worse", "different", "similar", "more", "less"
    ],
    "abstract_nouns": [
        "thing", "stuff", "issue", "matter", "aspect",
        "situation", "case", "topic", "subject"
    ],
    "discourse": [
        "again", "also", "as mentioned", "as discussed",
        "like before", "as above"
    ]
}

def rule_based_check(query: str, session_context=None):
    issues = []
    score = 0
    q_lower = query.lower()

    # --- Rule 1: Quá ngắn ---
    if len(query.split()) < 3:
        issues.append("too_short")
        score += 2

    # --- Rule 2: Match với AMBIGUITY_KEYWORDS ---
    for category, keywords in AMBIGUITY_KEYWORDS.items():
        for kw in keywords:
            if kw in q_lower:
                issues.append(f"keyword_{category}")
                score += 2
                break  # tránh duplicate trong cùng category

    # --- Rule 3: Regex cho câu hỏi yes/no thiếu subject ---
    if re.match(r"^(is it good\??|ok\??)$", q_lower):
        issues.append("missing_subject")
        score += 2

    # --- Rule 4: Phân tích cú pháp với spaCy ---
    doc = nlp(query)
    has_subject = any(token.dep_ in ["nsubj", "nsubjpass", "attr", "ccomp"] for token in doc)
    has_object = any(token.dep_ in ["dobj", "pobj"] for token in doc)
    has_main_verb = any(token.pos_ == "VERB" and token.dep_ == "ROOT" for token in doc)

    if not has_subject:
        issues.append("missing_subject")
        score += 1
    if not has_object:
        issues.append("missing_object")
        score += 1
    if not has_main_verb:
        issues.append("missing_main_verb")
        score += 1

    # --- Rule 5: Coreference nhưng không có antecedent trong session ---
    if any(pronoun in q_lower for pronoun in AMBIGUITY_KEYWORDS["pronouns"]):
        if not session_context or "entities" not in session_context:
            issues.append("unresolved_coreference")
            score += 1

    # Chuẩn hóa về thang 0–1
    normalized_score = score / 10

    return {
        "is_ambiguous": score > 3,
        "ambiguity_types": issues,
        "source": "rule",
        "rule_score": normalized_score
    }

def bert_classifier(query, model_classifier):
    result = model_classifier(query)
    label = result[0]['label']
    score = result[0]['score']
    is_ambiguous = not (label == "clear")
    return {
        "is_ambiguous": is_ambiguous,
        "ambiguity_types": ["bert_detect"],
        "source": "bert",
        "bert_score": score
    }

def detect_ambiguity(query: str, session_context=None, model_classifier=None) -> dict:
    # Step 1: Rule-based
    rule_result = rule_based_check(query, session_context)
    if rule_result["is_ambiguous"]:
        return {
            "is_ambiguous": True,
            "ambiguity_types": rule_result["ambiguity_types"],
            "source": "rule",
            "rule_score": rule_result["rule_score"]
        }

    # Step 2: BERT
    bert_result = bert_classifier(query, model_classifier)
    return {
        "is_ambiguous": bert_result["is_ambiguous"],
        "ambiguity_types": "bert_detect",
        "source": "bert",
        "bert_score": bert_result["bert_score"]
    }

def build_final_augmented_context(session, query_state, window_size):
    needed_keys = query_state["memory_usage"].get("needed_context_keys", [])
    session_summary = session.get("session_summary", {})
    working_messages = session.get("working_messages", [])

    session_memory = {}
    for key in needed_keys:
        parts = key.split(".")
        ref = session_summary
        for p in parts[1:]:
            if isinstance(ref, dict) and p in ref:
                ref = ref[p]
            else:
                ref = None
                break
        if ref:
            session_memory[p] = ref

    recent_messages = []
    if working_messages:
        sorted_msgs = sorted(working_messages, key=lambda m: int(m["id"]), reverse=True)
        recent_messages = sorted_msgs[:window_size]
        recent_messages = list(reversed(recent_messages))

    query_state["final_augmented_context"] = {
        "recent_messages": recent_messages,
        "session_memory": session_memory
    }

    return query_state

def build_rewrite_prompt(query_state):
    """
    Xây dựng prompt cho LLM để rewrite query dựa trên final_augmented_context và original_query.
    """

    original_query = query_state["original_query"]
    augmented_context = query_state.get("final_augmented_context", {})

    recent_messages = augmented_context.get("recent_messages", [])
    session_memory = augmented_context.get("session_memory", {})

    # Xác định chiến lược rewrite
    if recent_messages and session_memory:
        strategy = "combine_summary_and_working_messages"
    elif recent_messages:
        strategy = "use_working_messages"
    elif session_memory:
        strategy = "use_session_summary"
    else:
        strategy = "no_context"

    # Xây dựng prompt cho LLM
    prompt = f"""
You are tasked with rewriting an ambiguous user query into a clear, unambiguous form.

Original Query:
"{original_query}"

Augmented Context:
- Recent Messages: {recent_messages}
- Session Memory: {session_memory}

Rules:
- Use ONLY information explicitly present in the augmented context.
- Do NOT introduce new facts, assumptions, or details.

Task:
1. Rewrite the query using the augmented context so it becomes unambiguous.
2. Provide the rewritten query in JSON format:

{{
  "rewritten_query": "..."
}}
"""

    return prompt, strategy

def build_clarification_prompt(query_state):
    prompt_clarification = f"""
    You are tasked with generating clarifying questions for an ambiguous query.

    Rewritten Query: "{query_state["rewriting"]["rewritten_query"]}"
    Ambiguity Types: {query_state["analysis"]["ambiguity_types"]}

    Augmented Context:
    - Recent Messages: {query_state["final_augmented_context"]["recent_messages"]}
    - Session Memory: {query_state["final_augmented_context"]["session_memory"]}

    Rules:
    - Use ONLY information explicitly present in the augmented context.
    - Do NOT introduce new facts, assumptions, or details.

    Task:
    1. Generate 1 to 3 clarifying questions that would help the user specify their meaning.
    2. Rank the questions by usefulness (1 = most useful).
    3. Select the single best question to ask the user.
    4. Return in JSON format:
    {{
      "clarifying_questions": [
        {{"question": "...", "rank": }},
        {{"question": "...", "rank": }}
      ],
      "best_question": "..."
    }}
    """
    return prompt_clarification

def build_context(session, system_prompt, message):
    parts = []

    # System prompt
    if system_prompt:
        parts.append(f"[System Instruction]\n{system_prompt}")

    # User message hiện tại
    parts.append("[Current User Question]\n" + message)

    # User profile
    prefs = session["session_summary"]["user_profile"].get("prefs", [])
    constraints = session["session_summary"]["user_profile"].get("constraints", [])
    if prefs or constraints:
        profile_text = []
        if prefs:
            profile_text.append("Preferences: " + ", ".join(prefs))
        if constraints:
            profile_text.append("Constraints: " + ", ".join(constraints))
        parts.append("[User Information]\n" + "\n".join(profile_text))

    # Key facts
    key_facts = session["session_summary"].get("key_facts", [])
    if key_facts:
        parts.append("[Key Facts Known]\n" + "\n".join(key_facts))

    # Decisions
    decisions = session["session_summary"].get("decisions", [])
    if decisions:
        parts.append("[Decisions]\n" + "\n".join(decisions))

    # Open questions
    open_questions = session["session_summary"].get("open_questions", [])
    if open_questions:
        parts.append("[Open Questions]\n" + "\n".join(open_questions))

    # Todos
    todos = session["session_summary"].get("todos", [])
    if todos:
        parts.append("[Todos]\n" + "\n".join(todos))

    # Working messages
    log_text = convert_log_to_text(session["working_messages"])
    if log_text.strip():
        parts.append("[Recent conversations]\n" + log_text)

    parts.append("[End of Context]")
    # Ghép toàn bộ thành full_context
    full_context = "\n\n".join(parts)
    return full_context

def save_query_state(query_state, filepath="query_states.json"):
    if not Path(filepath).exists():
        with open(filepath, "w") as f:
            json.dump([], f)

    with open(filepath, "r") as f:
        data = json.load(f)

    data.append(query_state)

    # Ghi lại
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2)

"""# Langgraph

## AgentState
"""

# --- Session structures ---
class Metadata(TypedDict):
    session_id: str
    created_at: str
    last_activity_at: str
    ttl_seconds: int
    status: str
    token_budget: Optional[int]
    char_budget: Optional[int]
    window_size: Optional[int]


class UserProfile(TypedDict):
    prefs: List[str]
    constraints: List[str]


class SessionSummary(TypedDict):
    user_profile: UserProfile
    key_facts: List[str]
    decisions: List[str]
    open_questions: List[str]
    todos: List[str]


class MessageRangeSummarized(TypedDict):
    from_: Optional[str]
    to: Optional[str]


class SessionState(TypedDict):
    metadata: Metadata
    session_summary: SessionSummary
    message_range_summarized: MessageRangeSummarized
    working_messages: List[AnyMessage]


# --- Query structures ---
class Analysis(TypedDict):
    is_ambiguous: Optional[bool]
    ambiguity_types: List[str]
    source: str
    bert_score: Optional[float]
    rule_score: Optional[float]


class Rewriting(TypedDict):
    rewritten_query: str
    rewrite_strategy: List[str]


class MemoryUsage(TypedDict):
    needed_context_keys: List[str]
    memory_rationale: str


class Clarification(TypedDict):
    clarifying_questions: List[str]
    best_question: str
    question_generation_policy: str
    max_clarification_turns: int


class QueryState(TypedDict):
    session_id: str
    message_id: Optional[str]
    timestamp: str
    original_query: Optional[str]
    analysis: Analysis
    rewriting: Rewriting
    memory_usage: MemoryUsage
    clarification: Clarification
    final_augmented_context: Optional[str]


# --- AgentState for LangGraph ---
class AgentState(TypedDict):
    messages: Annotated[List[AnyMessage], add_messages]
    session: SessionState
    query_state: QueryState
    status: Literal["clear", "rewrite_clear", "clarification"]

# --- Init function ---
def init_agent_state(config: Dict[str, int],
                     session_id: str = "1",
                     message_id: Optional[str] = None,
                     user_query: Optional[str] = None) -> AgentState:
    now = str(datetime.now())

    state: AgentState = {
        "messages": [], # Initialize messages as an empty list
        "session": {
            "metadata": {
                "session_id": session_id,
                "created_at": now,
                "last_activity_at": now,
                "ttl_seconds": 36000,
                "status": "active",
                "token_budget": config.get("token_budget"),
                "char_budget": config.get("char_budget"),
                "window_size": config.get("window_size"),
            },
            "session_summary": {
                "user_profile": {"prefs": [], "constraints": []},
                "key_facts": [],
                "decisions": [],
                "open_questions": [],
                "todos": [],
            },
            "message_range_summarized": {"from_": None, "to": None},
            "working_messages": [],
        },
        "query_state": {
            "session_id": session_id,
            "message_id": message_id,
            "timestamp": now,
            "original_query": user_query,
            "analysis": {
                "is_ambiguous": None,
                "ambiguity_types": [],
                "source": "",
                "bert_score": None,
                "rule_score": None,
            },
            "rewriting": {
                "rewritten_query": "",
                "rewrite_strategy": [],
            },
            "memory_usage": {
                "needed_context_keys": [],
                "memory_rationale": "",
            },
            "clarification": {
                "clarifying_questions": [],
                "best_question": "",
                "question_generation_policy": "",
                "max_clarification_turns": 3,
            },
            "final_augmented_context": None,
        },
        "status": "clear",
    }

    return state

def init_query_node(state: AgentState):
    user_input = state["messages"][-1]
    session_id = state["session"]["metadata"]["session_id"]

    new_query_state = {
        "session_id": session_id,
        "message_id": getattr(user_input, 'id', "unknown"),
        "timestamp": str(datetime.now()),
        "original_query": user_input.content,
        "analysis": {
            "is_ambiguous": None,
            "ambiguity_types": [],
            "source": "",
            "bert_score": None,
            "rule_score": None
        },
        "rewriting": {"rewritten_query": "", "rewrite_strategy": []},
        "memory_usage": {"needed_context_keys": [], "memory_rationale": ""},
        "clarification": {
            "clarifying_questions": [],
            "best_question": "",
            "question_generation_policy": "",
            "max_clarification_turns": 0
        },
        "final_augmented_context": None
    }

    print(f"--- [System] Initialized fresh query state for: '{user_input.content[:30]}...' ---")

    return {"query_state": new_query_state}

"""## Node

### input_and_summary_node
"""

def input_and_summary_node(state: AgentState):
    user_input = state["messages"][-1].content
    conversation_text = convert_log_to_text(state["messages"])

    if should_trigger_summary(state["session"], conversation_text):
        summary_text = bart_summarize(model_summary, conversation_text)
        new_summary = call_llm(model_schema, build_prompt_extract_schema(summary_text))
        return {
            "session": {
                **state["session"],
                "session_summary": merge_summary(state["session"]["session_summary"], new_summary),
            },
            "messages": [RemoveMessage(id=m.id) for m in state["messages"][:-1]]
        }
    return {}

"""### check_ambiguity_node"""

def check_ambiguity_node(state: AgentState) -> Command[Literal["rewrite_node", "answer_node", "clarify_node"]]:
    query_state = state["query_state"]
    rewriting = query_state.get("rewriting", {})

    query_to_check = rewriting.get("rewritten_query") or state["messages"][-1].content
    is_second_check = bool(rewriting.get("rewritten_query"))

    detection_result = detect_ambiguity(query_to_check, model_classifier=model_classifier)
    is_ambiguous = detection_result["is_ambiguous"]

    new_analysis = {
        "is_ambiguous": is_ambiguous,
        "ambiguity_types": detection_result.get("ambiguity_types", []),
        "source": detection_result["source"],
        "bert_score": detection_result.get("bert_score"),
        "rule_score": detection_result.get("rule_score")
    }

    if not is_ambiguous:
        status = "rewrite_clear" if is_second_check else "clear"
        return Command(
            update={
                "query_state": {**query_state, "analysis": new_analysis},
                "status": status
            },
            goto="answer_node"
        )

    else:
        if not is_second_check:
            return Command(
                update={"query_state": {**query_state, "analysis": new_analysis}},
                goto="rewrite_node"
            )
        else:
            return Command(
                update={
                    "query_state": {**query_state, "analysis": new_analysis},
                    "status": "clarification"
                },
                goto="clarify_node"
            )

"""### rewrite_node"""

def rewrite_node(state: AgentState) -> Command[Literal["check_ambiguity_node"]]:
    session = state["session"]
    query_state = state["query_state"]

    query_state["memory_usage"]["needed_context_keys"] = [
        "session_summary.key_facts",
        "session_summary.open_questions"
    ]

    query_state = build_final_augmented_context(
        session,
        query_state,
        session["metadata"]["window_size"]
    )


    prompt, strategy = build_rewrite_prompt(query_state)

    res_rewritten = call_llm(model_rewrite, prompt)

    new_rewriting = {
        "rewritten_query": res_rewritten.get("rewritten_query", ""),
        "rewrite_strategy": [strategy]
    }

    return Command(
        update={
            "query_state": {
                **query_state,
                "rewriting": new_rewriting,
                "final_augmented_context": query_state["final_augmented_context"]
            }
        },
        goto="check_ambiguity_node"
    )

"""### clarify_node"""

def clarify_node(state: AgentState) -> Dict:
    query_state = state["query_state"]

    prompt_clarification = build_clarification_prompt(query_state)

    res_clarification = call_llm(model_clarification, prompt_clarification)

    clarifying_questions = res_clarification.get("clarifying_questions", [])
    best_question = res_clarification.get("best_question", "Could you please provide more details about your request?")

    new_clarification_data = {
        "clarifying_questions": [q["question"] if isinstance(q, dict) else q for q in clarifying_questions],
        "best_question": best_question,
        "question_generation_policy": "standard_clarification",
        "max_clarification_turns": 3
    }

    query_state["clarification"] = new_clarification_data

    save_query_state(query_state, filepath="query_logs_archived.json")
    print(f"--- [System] Clarification state archived for msg_id: {query_state.get('message_id')} ---")

    return Command(
        update={
            "query_state": query_state,
            "messages": [AIMessage(content=best_question)],
            "status": "clarification",
        },
        goto=END
    )

"""### answer_node"""

def answer_node(state: AgentState):
    session = state["session"]
    query_state = state["query_state"]

    final_query = query_state["rewriting"]["rewritten_query"] or state["messages"][-1].content

    full_context_prompt = build_context(
        session=session,
        system_prompt=system_prompt,
        message=final_query
    )

    response = model_answer.invoke(full_context_prompt)
    answer_content = response.content

    save_query_state(query_state, filepath="query_logs_archived.json")
    print(f"--- [System] Query state archived for msg_id: {query_state.get('message_id')} ---")

    return Command(
        update={
            "messages": [AIMessage(content=answer_content)],
            "status": state["status"]
        },
        goto=END
    )

"""## Edge"""

workflow = StateGraph(AgentState)

workflow.add_node("init_query_node", init_query_node)
workflow.add_node("input_and_summary_node", input_and_summary_node)
workflow.add_node("check_ambiguity_node", check_ambiguity_node)
workflow.add_node("rewrite_node", rewrite_node)
workflow.add_node("clarify_node", clarify_node)
workflow.add_node("answer_node", answer_node)

workflow.add_edge(START, "input_and_summary_node")
workflow.add_edge("input_and_summary_node", "init_query_node")
workflow.add_edge("init_query_node", "check_ambiguity_node")

"""## Build graph"""

# conn = sqlite3.connect("chat_memory.sqlite", check_same_thread=False)
# checkpointer = SqliteSaver(conn)
# app = workflow.compile(checkpointer=checkpointer)
app = workflow.compile()

"""# Run"""

def run_chat():
    config_values = load_config()
    thread_id = "session_001"
    thread_config = {"configurable": {"thread_id": thread_id}}

    initial_state = init_agent_state(config_values, session_id=thread_id)

    print("=== Demo Chat Assistant (LangGraph) ===")
    print("Type 'exit' to quit.")

    while True:
        u_input = input("\nUser: ")
        if u_input.lower().strip() == "exit": break

        input_data = {"messages": [HumanMessage(content=u_input)]}

        current_values = {}

        for mode, data in app.stream(input_data, thread_config, stream_mode=["debug", "values"]):

            if mode == "debug":
                event_type = data.get("type")
                if event_type == "task":
                    payload = data.get("payload", {})
                    print(f"[Debug] Node Starting: {payload.get('name')}")

            elif mode == "values":
                current_values = data

                summarized = current_values["session"].get("message_range_summarized", {})
                if summarized.get("to") is not None:
                    print("\n--- [System] Summarization Triggered! ---")
                    print(f"Summary Data: {json.dumps(current_values['session']['session_summary'], indent=2)}")

        if current_values:
            last_msg = current_values["messages"][-1].content
            status = current_values.get("status")

            if status == "clarification":
                print(f"Assistant (Clarification): {last_msg}")
            else:
                print(f"Assistant: {last_msg}")

    print("\n=== Final State Verification (from SQLite) ===")
    final_snapshot = app.get_state(thread_config)
    print(f"Last Activity: {final_snapshot.values['session']['metadata']['last_activity_at']}")
    print(f"Facts in Memory: {len(final_snapshot.values['session']['session_summary']['key_facts'])}")

    return final_snapshot

if __name__ == "__main__":
    state = run_chat()